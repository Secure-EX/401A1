5,6.63376217299e-177,6.20410227969e-177,4.74915837744e-235,1.26809441856e-212,1.3663214392e-267
10,3.11883094875e-144,5.79597460866e-127,2.03733496284e-105,6.63376217299e-177,6.20410227969e-177,4.74915837744e-235,2.56707030887e-110,1.26809441856e-212,1.3663214392e-267,4.45623728927e-124
20,2.3054900669e-99,3.11883094875e-144,5.79597460866e-127,2.03733496284e-105,6.63376217299e-177,5.42366619006e-84,7.73789910282e-89,1.30583002433e-97,6.20410227969e-177,4.74915837744e-235,2.67241855803e-86,5.57071901385e-98,2.56707030887e-110,4.13883748993e-83,3.05411703142e-82,1.26809441856e-212,1.3663214392e-267,4.45623728927e-124,8.81446003585e-88,7.94586848895e-90
30,2.3054900669e-99,3.11883094875e-144,5.79597460866e-127,2.03733496284e-105,6.63376217299e-177,5.42366619006e-84,7.73789910282e-89,7.46766940929e-71,3.12830823539e-63,1.30583002433e-97,6.20410227969e-177,2.73050076341e-76,4.74915837744e-235,2.67241855803e-86,1.01206810449e-60,5.45102307881e-81,5.57071901385e-98,2.56707030887e-110,4.13883748993e-83,1.21168782489e-70,3.05411703142e-82,1.07593897195e-74,1.26809441856e-212,1.3663214392e-267,4.77762291514e-79,1.56917178251e-80,4.45623728927e-124,8.81446003585e-88,7.05453207141e-74,7.94586848895e-90
40,2.3054900669e-99,3.11883094875e-144,5.79597460866e-127,2.03733496284e-105,6.63376217299e-177,5.42366619006e-84,4.21070685315e-57,7.73789910282e-89,2.88820507555e-53,7.46766940929e-71,3.12830823539e-63,2.0107337836e-55,1.30583002433e-97,6.20410227969e-177,3.14837933424e-60,2.73050076341e-76,3.29539744895e-54,4.74915837744e-235,3.80490973034e-57,2.67241855803e-86,1.01206810449e-60,5.45102307881e-81,5.57071901385e-98,2.56707030887e-110,4.13883748993e-83,1.07878479314e-57,1.21168782489e-70,3.05411703142e-82,1.07593897195e-74,1.26809441856e-212,1.14540686418e-52,1.3663214392e-267,2.56298581897e-57,1.94488574005e-57,4.77762291514e-79,1.56917178251e-80,4.45623728927e-124,8.81446003585e-88,7.05453207141e-74,7.94586848895e-90
50,2.3054900669e-99,3.11883094875e-144,5.79597460866e-127,2.03733496284e-105,6.63376217299e-177,5.42366619006e-84,4.21070685315e-57,7.73789910282e-89,2.88820507555e-53,7.46766940929e-71,7.67525418864e-45,3.12830823539e-63,2.0107337836e-55,1.30583002433e-97,6.20410227969e-177,3.14837933424e-60,1.89505095573e-52,7.63927856052e-45,2.73050076341e-76,3.29539744895e-54,4.74915837744e-235,3.80490973034e-57,2.67241855803e-86,1.01206810449e-60,5.45102307881e-81,4.69959429825e-49,6.83028037122e-48,1.23609989333e-44,5.57071901385e-98,2.00960767772e-42,2.56707030887e-110,4.13883748993e-83,1.07878479314e-57,1.21168782489e-70,3.05411703142e-82,1.07593897195e-74,1.26809441856e-212,1.14540686418e-52,1.3663214392e-267,2.56298581897e-57,5.29144895928e-47,1.94488574005e-57,4.77762291514e-79,1.56917178251e-80,4.45623728927e-124,8.81446003585e-88,7.05453207141e-74,1.10450169606e-46,1.55028646302e-43,7.94586848895e-90
0.324875,0.372625
I only analyzing top five features (k=5) that appear in both 1k dataset and 32k dataset. For my output which I converge them into a dictionary-like set with 1k: {10:2.35436243869e-07, 16:4.95221745856e-07, 110:1.85765415218e-06, 117:7.25518462552e-07, 160:3.04633384675e-06} and 32k: {21:6.63376217299e-177, 67:6.20410227969e-177, 110:4.74915837744e-235, 143:1.26809441856e-212, 149:1.3663214392e-267}.
3.3.3a) We can easily found that feature 110 (sorry I cannot get this feature's name) appears in both 1k dataset and 32k dataset.
3.3.3b) They both have different p-values and both smaller than 0.01, which is significantly rejected H0: no difference. It seems that the more data you input into the model, the smaller the p-value is. The reason is that the more data you get, the more noise you may omit, the model will become smoother.
3.3.3c) The reason why these features such as 21: "Standard deviation of AoA (100-700) from Bristol, Gilhooly, and Logie norm", etc. have been chosen in the 32k dataset is perhaps they have larger p-value than other feature's p-value.
